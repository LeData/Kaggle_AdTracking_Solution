'''
This module contains classes and functions for feature engineering

FeatureCrawler (class):
Uses graphs to manages and explore a (discrete) feature space. Automatically removes whole areas of the space that do not contain a local max.
  - Methods:
        update_graph(): takes a list of features and expands the feature space if new features are found
        get_unscored_node(): returns a feature set that has not been scored yet.
        record score(): takes in the score of a list of features and update the graph.
        prune(): removes all scored nodes in the periphery of the graph that aren't a global max. This should only be used if looking for 
                a single model. Not good for blending.
    - Attributes:
        status_ : percentage crawled / current proportion of scored nodes in the graph
        leaves_ : {current peripheral nodes that were scored : their score}

FeatureManager (class):
Manages feature creation and storage for fast loading and easy management through a configuration file. Currently only dealing with Binary Classification.
    Methods:
        set_downsample() - Turns on downsampling and generates a sample index.
        get_training_data() -
        get_test_data() -
        get_all_data() -
        update_features() - update the feature list from the config file.
         
    Attributes:
    feature_list_: simple list of features. the name of the features is generated by the feature generator and corresponds to the file on disk.
    downsample_: boolean indicating the use of downsampling
'''

## TODO:
## 1 - Finish update_types() parquet 1.0 returns int64 for uint32
## 2 - add verbose option
## 3 - avoid downsampling when it is impossible


import pandas as pd
import numpy as np
import networkx as nx
import random
import yaml
import os
import gc
import inspect
from itertools import chain
import generators

class FeatureCrawler(object):
    # This class is a graph crawler for feature engineering. It maintains a graph of all sets of features,
    # their respective inclusions and scores. It incrementally learns the performance of feature sets to find the best model.
    # It can expand the feature graph at any time with new features and automatically removes feature combinations that will not improve the model
    # An option to prune all non optimal branches is provided.

    null_set=set()
    unit_graph=nx.DiGraph()
    unit_graph.add_edge('s','t')
    nx.set_node_attributes(unit_graph,{'s':null_set,'t':null_set},'feats')
    nx.set_node_attributes(unit_graph,{'s':True,'t':False},'score')
    
    def __init__(self,file_path,crawler_file,incr_score=True):
        # initiate with a baseline and score order (incr, decr)
        self.status_=0
        self.leaves_={}
        self.file=file_path+crawler_file+'.yaml'
        self.gt=incr_score # defines whether better scores are defined with the greater than or lesser than operator.
        
        if os.path.isfile(self.file):
            print('loading graph from file')
            self.G=nx.read_yaml(self.file)
            self.__update_status()
        else:
            print('creating empty feature graph')
            self.G=nx.DiGraph()
            self.G.add_node(0,feats=self.null_set,score=None)

    def __force_list(self,*arg):
        ''' Takes a list of arguments and returns the same, 
        but where all items were forced to a list.

        example : list_1,list_2=__force_list(item1,item2)
        '''
        Gen=(x if isinstance(x,list) else [x] for x in arg)
        return Gen if len(arg)>1 else next(Gen)

    def __is_better(self,a,b):
        # returns True if a is better than b.
        if a is None:
            return False
        elif b is None:
            return True
        return a>b if self.gt else a<b

    def __update_status(self):
        #getting the proportion of non empty feature sets evaluated
        self.status_=np.mean([y is not None for x,y in nx.get_node_attributes(self.G,'score').items() if x!=0])
        self.leaves_={n:self.G.node[n]['score'] for n,deg in self.G.out_degree if deg==0 and self.G.node[n]['score'] is not None}
        nx.write_yaml(self.G,self.file)
        #print('Crawler status: progress: {0.status_}, leaves: {0.leaves_}.'.format(self))
        return self

    def update_graph(self,feature_list):
        ''' Takes a list of available features and updates the graph with the ones that weren't included yet
            Args- list of features
        '''
        feature_list=self.__force_list(feature_list)
        current_features=list(chain(*[features for n,features in nx.get_node_attributes(self.G,'feats').items() if len(features)==1]))
        new_features=[x for x in feature_list if x not in current_features]
        for x in new_features:
            print('Adding feature {} to the feature space.'.format(x))
            self.unit_graph.node['t']['feats']={x}
            self.G=nx.algorithms.operators.cartesian_product(self.G,self.unit_graph)
            self.unit_graph.node['t']['feats']=self.null_set
            new_labels={x:y[0]|y[1] for x,y in nx.get_node_attributes(self.G,'feats').items()}
            nx.set_node_attributes(self.G,new_labels,'feats')
            
            new_scores={x:y[0] if y[1] else None for x,y in nx.get_node_attributes(self.G,'score').items()}
            nx.set_node_attributes(self.G,new_scores,'score')
            
            self.G=nx.convert_node_labels_to_integers(self.G)

        self.__update_status()
        return self

    def get_unscored_node(self):
        if self.status_==1:
            print('All features were estimated, returning a random leaf')
            node=random.choice(list(self.leaves_))
        # the graph G may already be topologically ordered from 0 by the renaming of the nodes
        else:
            unscored_nodes=[n for n,score in nx.get_node_attributes(self.G,'score').items() if (score is None) and (n>0)]
            unscored_subgraph=self.G.subgraph(unscored_nodes)
            choices=[n for n,deg in unscored_subgraph.in_degree if deg==0]
            node=random.choice(choices)
        return {'node':node,**self.G.node[node]}

    def record_score(self,node_dict):
        # checks that the score that is about to be recorded corresponds to the given features
        # adds it to the graph and removes node and all descendants if  the score wasn't improved from all its predecessors.
        node=node_dict.pop('node')
        if self.G.node[node]['feats']!=node_dict.pop('feats'):
             print('The features do not correspond to the given node')
        else:
            self.G.node[node]['score']=node_dict.pop('score')
            if any([self.__is_better(self.G.node[x]['score'],self.G.node[node]['score']) for x in nx.ancestors(self.G,node)]):
                self.G.remove_nodes_from({node}|nx.descendants(self.G,node))
        self.__update_status()
        return self

    def check_condition(self,condition):
        # condition must be {'number': int, 'threshold': float}. It asks for at least n leaves above a certain threshold.
        num_leaves=len(self.leaves_)
        print('The crawler explored {}% of the feature space.'.format(self.status_*100))
        print('There are {} leaves available.'.format(num_leaves))
        if self.status_==1:                
            return True
        else:
            return len({x:y for x,y in self.leaves_.items() if self.__is_better(y,condition['threshold'])})>=condition['number']

    def prune(self):
        # Recursively removes all scored leaves that are not global max
        # to use only if needing the single best model, not good for bagging/blending.
        while len({score for leaf,score in self.leaves_.items()})>1:
            max_score=max(nx.get_node_attributes(self.G,'score').items(), key=operator.itemgetter(1))[1]
            suboptimal_features={feats for feats,score in self.leaves_.items() if self.__is_better(max_score,score)}
            suboptimal_leaves=[n for n,feat in nx.get_node_attributes(self.G,'feats') if feats in suboptimal_features]
            self.G.remove_nodes_from(suboptimal_leaves)
            self.__update_status()          
        return self

class FeatureManager(generators.FeatureGenerators):

    def __init__(self,feature_path='features/',config_path='',seed=714):
        # keeps a ditionary of available feature generators
        self.feature_path=feature_path
        self.feature_list_=[]
        self.raw_index=[]
        self.target_feature=None
        self.submission_feature=None
        self.sample_index=self.raw_index
        self.downsample_=False
        self.seed=seed
        self.feature_generators={x:y for  x,y in inspect.getmembers(self, predicate=inspect.ismethod) if x.startswith('feat_')}
        self.dict_file='{}FeatureManager_config.yaml'.format(config_path)

        if os.path.isfile(self.dict_file):
            print('loading feature dict')
            feature_dict=self.update_features()
        else:
            print('No config file in {}, generating initial features.'.format(config_path))
            feature_dict=self.feat_initial()
        self.__save_feature_dict(feature_dict)

    def __force_list(*arg):
        ''' Takes a list of arguments and returns the same, 
        but where all items were forced to a list.

        example : list_1,list_2=__force_list(item1,item2)
        '''
        Gen=(x if isinstance(x,list) else [x] for x in arg)
        return Gen if len(arg)>1 else next(Gen)

    def update_features(self):
        '''
            The feature dictionary is of the format
            {'method_name': {feature_name: kwargs}
            it is saved in yaml and parsed for new features. Any method name that doesn't exist or start with 'feat_' will be dropped.
            Initial features are a dict key:None except for target and test/subscription features.
            To add new features, manually add to the file '{method: {new_XX: kwargs}}' where XX is an integer
        '''
        with open(self.dict_file,'r') as File:
            temp_dict=yaml.load(File)
        #removing instructions
        temp_dict={x:y for x,y in temp_dict.items() if x in self.feature_generators}
        for method in temp_dict:
            if method!='feat_initial':
                new_features={}
                for feature in temp_dict[method]:
                    if feature.beginswith('new_'):
                        #check for duplicates, we don't want to compute something that already exists 
                        kwargs=temp_dict[method].pop(feature)
                        new_feature=self.feature_generators[method](**kwargs)
                        new_features[new_feature.name]=kwargs

                        print('saving parquet file')
                        new_feature.to_parquet('{}{}.pqt'.format(self.feature_path,feature_name))
                        del(new_feature);gc.collect()
                temp_dict[method].update(new_features)
        return temp_dict

    def __save_feature_dict(self,feat_dict):

        feat_dict['instructions']='Add new features by adding a row to the table of the function you \'re using, with the arguments you want and \'new_XX\' where XX is an integer as feature_name.'
        with open(self.dict_file,'w') as File:
            yaml.dump(feat_dict,File)
        feat_dict.pop('instructions')
        #extracting raw index, target and sub file names before collecting the remaining features in an iterator
        extras={feat_dict['feat_initial'].pop(x):x for x,y in list(feat_dict['feat_initial'].items()) if y is not None}
        self.raw_index=pd.RangeIndex(int(extras['index']))
        self.target_feature=extras['target']
        self.submission_feature=extras['sub']
        self.feature_list_=list(chain(*[y for x,y in feat_dict.items()]))
        return self

    def __get_series(self,feature):
        if feature=='sub':
            feature=self.submission_feature
        elif feature=='target':
            feature=self.target_feature
        elif feature not in self.feature_list_:
            print('feature not available')
            return None

        feature_parquet='{}{}.pqt'.format(self.feature_path,feature)
        #print('loading from {}'.format(feature_parquet))
        with open(feature_parquet,'rb') as File:
            feature_series=pd.read_parquet(File) # index cannot be a range index here until all features have been merged.
        return feature_series.iloc[:,0]

    def __get_dataframe(self,features,index=None):
        reset=False
        if index is None:
            reset=True
            index=self.raw_index
        df=pd.DataFrame(index=index)
        for feat in features:
            if feat not in self.feature_list_:
                print('feature not available')
            else:
                df=df.join(self.__get_series(feat))
        if reset:
            df=df.reset_index(drop=True) #int to range index
        return df

    def __get_ML_data(self,features,test=False):
        self.__force_list(features)
        if test:
            extra=self.__get_series('sub')
        else:
            extra=self.__get_series('target')
            if self.downsample_:
                extra=extra.loc[self.sample_index]
        
        df=self.__get_dataframe(features,extra.index)
        df=self.update_types(df)
        return df,extra

    def update_types(self,data):
        data=data
        return data

    def set_downsample(self,prop=.2):
        self.downsample_=True
        y=self.__get_series('target') # beware, there may be an issue here if train and test_supplement overlap in time.
        if not 0 < prop <1:
          print('Downsampling proportion not valid, using .2')
          prop=.2
        n=np.sum(~y)-np.sum(y)*(1/prop -1)
        self.sample_index=(y.
            drop(y[~y]
                .sample(n=int(n),random_state=self.seed)
                .index)
            .index)
        return self

    def get_all_data(self,features):
        return self.__get_dataframe(features)

    def get_training_data(self,features):
        return self.__get_ML_data(features,False)

    def get_test_data(self,features):
        return self.__get_ML_data(features,True)

'''   def __feat_initial(self):
       # target and test/submission files must only be of the corresponding 
       # index in the main dataframe
       # print('making parquet files of the dataset for quicker loading')
       def load_csv(name):

           file_csv='{}{}.csv'.format(self.feature_path,name)

           # Defining dtypes
           types = {}

           # Defining csv file reading parameters
           read_args={
               'dtype':types,
               }

           print('Loading {}'.format(file_csv))
           with open(file_csv,'rb') as File:
               df=pd.read_csv(File,**read_args)
           print(df.info())
           return df

       print('Loading data')
       X=load_csv('exercise')

       print('saving featues, raw index, target and submission to parquet files')
       initial_features={str(len(X.index)):'index'}
       for col in X:
           if col == 'Class':
               initial_features[col]='target'
               X.pop(col)[lambda x: x>=0].astype(bool).to_frame().to_parquet('{}{}.pqt'.format(self.feature_path,col))
           elif col == 'test_id':
               initial_features[col]='sub'
               X.pop(col)[lambda x: x>=0].astype(np.uint32).to_frame().to_parquet('{}{}.pqt'.format(self.feature_path,col)) #note that parquet 1.0 doesn't keep int32 anyhow, so it'll be int64
           else:
               initial_features[col]=None
               X.pop(col).fillna(0).to_frame().to_parquet('{}{}.pqt'.format(self.feature_path,col))
       del(X);gc.collect()
       return {'feat_initial':initial_features}

'''